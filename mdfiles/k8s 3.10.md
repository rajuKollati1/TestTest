## Steps - Upgrade Kubernetes From v1.29.9 to v1.30.10 and Calico Version Upgrade.
 
**PREPARATION**
 
  * Get downtime slot from MAN-IT/Tech Manager via Email and Create Change Request accordingly. Change need to be Start Implementation If Change Status should be **Deploy State** (all approvals received).
  * Required nodes(sites) should be **Muted** (suppress alerts by SCOM Team) before start of activity.
  * Disable Cronjob (MQTT heartbeat producer) press s to suspend. It should be Suspend:True now.
    ![image](https://github.com/user-attachments/assets/faa62b22-7aaa-4786-8e2c-4fa6db441e31)
  * **Backup** need to be taken 1 hour Prior to activity
  * **Snapshot** need to be taken just before the activity (Incase if fall back required -we can ask Wintel team to rollback to snapshot)
  * Take Screenshot of MQTT Dashboard for that location.
 
```console
df -h /
```
In case the utilization is more than 80%, consider cleaning up older backups of ETCD present in "/etc/kubernetes/tmp".
 
Example, see below to check and delete one of the older ETCD backups:
```console
root@frabbvelk8mp101:~# df -h /
Filesystem              Size  Used Avail Use% Mounted on
/dev/mapper/vg00-lvol1  6.9G  5.6G  894M  87% /
root@frabbvelk8mp101:~#
root@frabbvelk8mp101:~# cd /etc/kubernetes/tmp/
root@frabbvelk8mp101:/etc/kubernetes/tmp#
root@frabbvelk8mp101:/etc/kubernetes/tmp# du -sh *
855M    kubeadm-backup-etcd-2022-11-24-15-14-27
855M    kubeadm-backup-etcd-2023-01-25-08-35-48
24K     kubeadm-backup-manifests-2021-07-22-07-45-41
16K     kubeadm-backup-manifests-2021-07-22-08-26-59
24K     kubeadm-backup-manifests-2022-11-24-11-56-06
20K     kubeadm-backup-manifests-2022-11-24-15-14-27
20K     kubeadm-backup-manifests-2023-01-25-08-35-48
root@frabbvelk8mp101:/etc/kubernetes/tmp#
root@frabbvelk8mp101:/etc/kubernetes/tmp# rm -rf kubeadm-backup-etcd-2022-11-24-15-14-27
root@frabbvelk8mp101:/etc/kubernetes/tmp#
root@frabbvelk8mp101:/etc/kubernetes/tmp# df -h /
Filesystem              Size  Used Avail Use% Mounted on
/dev/mapper/vg00-lvol1  6.9G  4.8G  1.8G  74% /
root@frabbvelk8mp101:/etc/kubernetes/tmp#
```
 
- ***Only applicable for CHNLA:*** For CHNLA cluster, we need to unset the proxy settings on all the nodes before starting the upgrade. We do not need to set it again as it is set for only the current session.
 
```console
unset http_proxy
unset https_proxy
unset no_proxy
```
 
- **Mandatory Step:** On all Master and Worker nodes, perform the below steps to update the needed repositories from where the needed software can be downloaded and installed:
 
*For both Kubernetes and ContainerD Software:*
```console
rm /etc/apt/sources.list.d/kubernetes.list
rm /etc/apt/keyrings/kubernetes-apt-keyring*
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" > /etc/apt/sources.list.d/kubernetes.list
curl -fsSL -k https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
rm /etc/apt/sources.list.d/docker.list
rm /etc/apt/keyrings/docker.gpg
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg && chmod a+r /etc/apt/keyrings/docker.gpg && echo "deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
apt-get update

```
Validate required package has been download successfully
```console
apt-cache madison kubeadm
```
**Note:** "kubectl" commands should either be run only on the first master node or from the tool server, for example, after connecting to the cluster via the kubeconfig.
 
**KUBEADM and Control-Plane upgrade on all Master and Worker Nodes**
 
**NOTE**: If it's mentioned to run commands at the same time, then copy/paste/run those commands on all the nodes required without waiting for the commands execution to complete.
 
**1)** Run the below commands on all master and worker nodes, at the same time:
 
Check and remove hold, if any, on kubeadm:
 
```console
apt-mark showhold
apt-mark unhold kubeadm
 
```
 
**2)** Run the below commands on all master and worker nodes, at the same time:
 
Upgrade kubeadm to the needed version and check the version after installation:
 
```console
apt-get install -y kubeadm=1.30.10-1.1
 
kubeadm version
apt-mark hold kubeadm
 
```
 
**3)** Run the below command, only on first master node as a pre-task. This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to.
 
**ONLY ON FIRST MASTER NODE**
 
```console
kubeadm upgrade plan
 
```
 
**4)** Run the below command only on the first master node, to upgrade control plane components:
 
**ONLY ON FIRST MASTER NODE**
 
```console
kubeadm upgrade apply v1.30.10
 
```


```

Validate required package has been download successfully

```console

apt-cache madison kubeadm

```

**Note:** "kubectl" commands should either be run only on the first master node or from the tool server, for example, after connecting to the cluster via the kubeconfig.
 
**KUBEADM and Control-Plane upgrade on all Master and Worker Nodes**
 
**NOTE**: If it's mentioned to run commands at the same time, then copy/paste/run those commands on all the nodes required without waiting for the commands execution to complete.
 
**1)** Run the below commands on all master and worker nodes, at the same time:
 
Check and remove hold, if any, on kubeadm:
 
```console

apt-mark showhold

apt-mark unhold kubeadm
 
```
 
**2)** Run the below commands on all master and worker nodes, at the same time:
 
Upgrade kubeadm to the needed version and check the version after installation:
 
```console

apt-get install -y kubeadm=1.30.10-1.1
 
kubeadm version

apt-mark hold kubeadm
 
```
 
**3)** Run the below command, only on first master node as a pre-task. This command checks that your cluster can be upgraded, and fetches the versions you can upgrade to.
 
**ONLY ON FIRST MASTER NODE**
 
```console

kubeadm upgrade plan
 
```
 
**4)** Run the below command only on the first master node, to upgrade control plane components:
 
**ONLY ON FIRST MASTER NODE**
 
```console

kubeadm upgrade apply v1.30.10
 
```
 
**5)** Run the below command on all the remaining master and all worker nodes, ONLY ONE AT A TIME, starting with the 2nd master node and ending with the 3rd worker node:
 
```console

kubeadm upgrade node
 
```

**Note:** **Before move forward to step 6 ensure all pods should be UP ( specially kyverno)**
 
```console

kubectl get pods -A 

kubectl get pods -n kube-addons | findstr kyverno
 
if pods are in error state. Describe the pods ,if errors related to calico authorization -follow below step:
 
kubectl get pods -A | egrep -i "calico|coredns|kube-proxy" | awk '{print "-n "$1" "$2}' | xargs -L 1 -r kubectl delete pod --force
 
Check all pods status now, it should be running now (specially kyverno-admission-controller)
 
kubectl get pods -A
 
```
 
**6) Only one node at a time, Start with the first master node and end with the 3rd worker node .Drain and update kubelet, kubectl and containerd**
 
```console

apt-mark unhold kubelet kubectl
 
kubectl drain <node> --ignore-daemonsets --delete-emptydir-data --force

```
 
If you notice that the drain command is hung, then there are two possibilities:  

- There are pods in Terminating state on the specific node. To speed up the deletion of such pods, run the below command:

```console

kubectl get pods -A | grep Terminating | awk '{print "-n "$1" "$2}' | xargs -L 1 -r kubectl delete pod --force
 
```

- There are pods that are unable to be evicted because there is a PDB configured. Run the below script to delete such pods running on the specific node:

```console

# To be done on the tool server via GitBash:

kubectx <context>
 
# Ensure that the correct context is set:

kubectl get nodes
 
# Run the below commands:

cd /e/VELUX-HCL-DevOps/Scripts

sh delete_pdb_pods.sh <worker node name>

```
 
```console

apt-get install -y kubelet=1.30.10-1.1 kubectl=1.30.10-1.1
 
apt-mark hold kubelet kubectl

systemctl daemon-reload

systemctl restart kubelet
 
systemctl stop kubelet
 
# If "kubelet" stops within two seconds, move to next stop with stopping "containerd".

# Else, reboot the node with "shutdown -r now".

# Once it is up, stop kubelet with "systemctl stop kubelet" and then proceed with next step with stopping "containerd".

```
 
```console

systemctl stop containerd
 
apt-mark unhold containerd.io

apt-get install -y containerd.io=1.7.24-1
 
apt-mark hold containerd.io

systemctl daemon-reload

systemctl start kubelet
 
kubectl uncordon <node>

```
 
**7) Validation**
 
Post all nodes updated. All the nodes should be in Ready state and the Kubernetes version should be v1.30.10
 
kubectl get nodes -o wide
 
## Part 2:-------Calico Version Upgrade------
 
**Step 1)** Open this link  [Click here](https://github.com/VELUX/hcl-operations-docs/blob/main/docs/playbook/Calico_upgrade_v3.29.3.yaml)
 
**Step 2)** In First ** Master node** create file Calico_upgrade_v3.29.3.yaml and copy-paste all content. -Save
 


```console
 
vi Calico_upgrade_v3.29.3.yaml
 
flux suspend ks calico -n flux-system
 
kubectl apply --server-side --force-conflicts -f Calico_upgrade_v3.29.3.yaml

```
 
Wait for everything to come up normally:

-- All K8S nodes are in "Ready" state.

-- All "calico-node" daemonset pods are in a Running state (see if there are frequent restarts)

-- "calico-kube-controller" should be in a running state.

-- Verify kube-apiserver, kube-proxy, DNS pods are all working as expected.

-- Verify all other pods in all other namespaces are running fine. 

-- Perform all other verifications.
 
**Step 3)** Update the original upgrade manifest to GitHub
 
[Copy File](https://github.com/VELUX/hcl-operations-docs/blob/main/docs/playbook/Calico_original.yaml) and replace in  https://github.com/VELUX/ccoe-platform/blob/main/kube-addons/czevy/calico/calico.yaml (ex : This is for CZEVY)
 
Create, PR, get it merged etc.
 
**Step 4)** Resume the suspended kustomization,Ensure you are on the correct context.
 
```console

flux resume ks calico -n flux-system
 
rm Calico_upgrade_v3.29.3.yaml

```

------------------

**Validate Service Account custom OIDC Issuer URL**
 
 
**1)** On all master nodes of the K8S Cluster, value of the "--service-account-issuer" in the path: cat /etc/kubernetes/manifests/kube-apiserver.yaml it should be as below.
 
```console

- --service-account-issuer=https://selfhostoidc.blob.core.windows.net/<location code>

```

***Note: If this does not match with above, Update the following line if required***
 
```console

- --service-account-issuer=https://kubernetes.default.svc.cluster.local

TO

- --service-account-issuer=https://selfhostoidc.blob.core.windows.net/<location code>

```
 
**2)** Restart all the Calico, coredns and kube-proxy pods:
 
```console

kubectl get pods -A | egrep -i "calico|coredns|kube-proxy" | awk '{print "-n "$1" "$2}' | xargs -L 1 -r kubectl delete pod --force

```
 
Wait for all these deleted pods to come up. Keep running the below command until you see all the pods are up:

 


```console

kubectl get pods -A | egrep -i "calico|coredns|kube-proxy"

```
 
**3)** Enable Cronjob (MQTT heartbeat producer) press s to Resume. It should be Suspend :False now.
 
     Verify all 3 heartbeat producers to ensure they all are running as expected,

    ![image](https://github.com/user-attachments/assets/faa62b22-7aaa-4786-8e2c-4fa6db441e31)
 
This can be done via K9S for ease of doing it.
 
**4)** Proceed with post-verification steps before concluding the activity.
 
**Clean-up repositories added for both Kubernetes and docker from all master and worker nodes**
 
After successful upgrades, remove the source list for both Kubernetes and docker along with the added keyrings.
 
In the future, when a new upgrade is needed, these can be set-up as needed for the newer versions.
 
```console

rm /usr/share/keyrings/kubernetes-archive-keyring.gpg

rm /etc/apt/keyrings/docker.gpg

rm /etc/apt/keyrings/kubernetes-apt-keyring*

rm /etc/apt/sources.list.d/kubernetes.list

rm /etc/apt/sources.list.d/docker.list

apt-get update

ls -al /etc/apt/sources.list.d

```
 
**5) Verify K8s package hold Status . (Check- All master and worker nodes- you should received 4 results, containerd.io, kubeadm, kubectl,kubelet)**
 
```console

apt-mark showhold

```

**If results is less than 4, Validate and set missing package on hold (In below Example: Kubeadm is missing)**
 
![image](https://github.com/user-attachments/assets/f93d426f-c94a-45e2-b59f-a56acc1b7b27)
 
```console

apt-mark hold kubeadm

```
 
***Note:*** If Upgrade not went Successful - Take commands History, Nodes screenshots, error ,logs etc.
 
***Fall back Plan:*** Ask Wintel team to revert the snapshot.
 
**Validation:-** Make sure all pods should be UP after upgrade ,if we have any pods down in VIP-CORE-MQTT namespace. we need to resolve immediately .
 
    Note :-Check MQTT dashboard should up UP and running . If it 502 error, it might be Emqx-core pod is down.
 
**UseCase 1)** Try to Full Restart emqx-core via statefulset < Edit the statefulset -> Make replicas 1 to 0 >, next step - Replicasets > emqx-replicants < Make replicas 1 

    to 0> . to see if all pods in vip-core-mqtt namesapce is running now. If still any pod is not running follow next step.
 
**UseCase 2)** check logs of emqx replicant pod -if any error related missing acl configuration.

  In Prerequisites step - you have copy content of acl file before the activity. Replace with current acl file.
 
**Usecase 3)** In logs if there is any pod lock issue .follow below runbook to solve the issue.
 
   https://github.com/VELUX/hcl-operations-docs/blob/main/docs/runbook/pod-lock-issue.md

**Usecase 4)** If MQTT dashboard is UP but Topics/connectors No Data . Check if all source connectors are down.
 
   Restart source connectors from Control Center solve the problem.
 
**Usecase 5)** If hivemq-(0/1) is in Pending state and Node name is n/a (not assigned)
 
   Describe the pod if events/errors related to 0/6 Nodes are unavailable. Check high utilization pod and kill it will free some memory/cpu and hivemq will get node to start running.
 
